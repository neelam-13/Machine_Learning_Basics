{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw3K_GqWSYml"
      },
      "source": [
        "# Exercise 4: Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeS-Wg8ySYm2"
      },
      "source": [
        "## Exercise 4.1: \n",
        "\n",
        "In this exercise, we will use the adaptive boosting algorithm [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) to combine multiple weak classification models to one strong classifier for a two-class classification problem\n",
        "on a two-dimensional dataset. Our simple classifier will be a threshold on either dimension, dividing the dataset into two classes.\n",
        "The data will be generated with the provided function genData. Each row in the generated matrix data corresponds to one observation and each column to a single feature. The generated vector labels contains the corresponding desired output value of the two-class classification problem. \n",
        "\n",
        "When done correctly, your code should do the following:\n",
        "1. Create a random dataset.\n",
        "2. Iterativly, do the following:\n",
        "    <br>a) Train another weak classification model.\n",
        "    <br>b) Compute the predictions of the weak classifier.\n",
        "    <br>c) Calculate the weighted training error of the weak classifier.\n",
        "    <br>d) Update the distribution of weights assigned to each data sample.\n",
        "    <br>e) Combine all the weak classification models to one strong classifier.\n",
        "    <br>f) Calculate the error of the strong classifier.\n",
        "    <br>g) Compute the decision boundary of the strong classifier.\n",
        "3. Plot the error of the strong classifier for each iteration.\n",
        "4. Create an animation which shows how the decision boundary of the strong classifiers changes over time.\n",
        "\n",
        "In case you are struggeling with the task, here are some helpful tips and hints:\n",
        "1. Useful functions : gen_data, train_classifier, get_decision_boundary\n",
        "2. Good initial values are: n_samples=2000, n_classifiers=200.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmf3Fac7SYm6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "%matplotlib inline\n",
        "\n",
        "def gen_data(n_samples):\n",
        "    \"\"\"\n",
        "    Generate data.\n",
        "    \n",
        "    INPUT:\n",
        "        n_samples : number of samples to generate (observations)\n",
        "        \n",
        "    OUTPUT:\n",
        "       data : 2D data points\n",
        "       labels : ndarray of class labels with integer values -1 or 1\n",
        "    \"\"\"\n",
        "    \n",
        "    data = np.random.random((n_samples,2)) - 0.5\n",
        "    labels = np.linalg.norm(data, axis=1) < 0.4\n",
        "    labels = (labels.astype(int) * 2) - 1\n",
        "    \n",
        "    return data, labels\n",
        "\n",
        "class Classifier(object):\n",
        "    \n",
        "    def __init__(self, dimension=0, threshold=0., sign=1):\n",
        "        super().__init__()\n",
        "        self.dimension = dimension\n",
        "        self.threshold = threshold\n",
        "        self.sign = sign\n",
        "        \n",
        "    def predict(self, data):\n",
        "        \"\"\"\n",
        "        Predict new data\n",
        "\n",
        "        INPUT:\n",
        "            data : 2D data points\n",
        "\n",
        "        OUTPUT:\n",
        "            estimated labels\n",
        "        \"\"\"\n",
        "        return self.sign * ((data[:, self.dimension] >= self.threshold).astype(int) * 2 - 1)\n",
        "    \n",
        "def train_classifier(classifier, data, targets, weights):\n",
        "    \"\"\"\n",
        "    Predict new data\n",
        "\n",
        "    INPUT:\n",
        "        classifier : classifier to train\n",
        "        data : 2D data points\n",
        "        targets : ndarray of class labels with integer values -1 or 1\n",
        "        weights: weights assigned to each data sample\n",
        "\n",
        "    OUTPUT:\n",
        "        classifier : trained classifier\n",
        "    \"\"\"\n",
        "    min_error = np.inf\n",
        "    dimensions = [0, 1]\n",
        "    thresholds = np.linspace(-0.5, 0.5, 11)\n",
        "    signs = [-1, 1]\n",
        "    best_parameters = (0, 0., 1)\n",
        "\n",
        "    for d in dimensions:\n",
        "        classifier.dimension = d\n",
        "        for t in thresholds:\n",
        "            classifier.threshold = t\n",
        "            for s in signs:\n",
        "                classifier.sign = s\n",
        "                \n",
        "                predictions = classifier.predict(data)\n",
        "                error = (weights * (predictions != targets).astype(int)).sum()\n",
        "                \n",
        "                if error < min_error:\n",
        "                    best_parameters = (d, t, s)\n",
        "                    min_error = error\n",
        "    \n",
        "    classifier.dimension = best_parameters[0]\n",
        "    classifier.threshold = best_parameters[1]\n",
        "    classifier.sign = best_parameters[2]\n",
        "    return classifier\n",
        "\n",
        "class Animation(object):\n",
        "    \"\"\"\n",
        "    Creates animation object to visualize the decision boundaries of the strong classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels, decision_boundaries):\n",
        "        \"\"\"\n",
        "        INPUT:\n",
        "            data : 2D data points\n",
        "            labels : ndarray of class labels with integer values -1 or 1\n",
        "            decision_boundaries: ndarray of shape (n_classifiers, n**2) with prediction results for an (n x n) grid.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.decision_boundaries = decision_boundaries\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        \n",
        "        # plot data points\n",
        "        ax.scatter(data[labels==-1,1], data[labels==-1,0], c=np.array([[0.5, 0.7, 1.]]), marker='o')\n",
        "        ax.scatter(data[labels==1,1], data[labels==1,0], c=np.array([[1., 0.6, 0.6]]), marker='x')\n",
        "        # write axis labels\n",
        "        ax.set_xlabel('x')\n",
        "        ax.set_ylabel('y')\n",
        "        \n",
        "        x_lim = ax.get_xlim()\n",
        "        y_lim = ax.get_ylim()\n",
        "        im = ax.imshow(decision_boundaries[0], extent=(-0.55,0.55,-0.55,0.55), interpolation='none', cmap='jet', origin='lower', vmin=-1, vmax=1)\n",
        "        ax.set_xlim(x_lim)\n",
        "        ax.set_ylim(y_lim)\n",
        "        \n",
        "        fig.colorbar(im)\n",
        "        \n",
        "        self.fig = fig\n",
        "        self.im = im\n",
        "        \n",
        "    def animate(self, i):\n",
        "        \"\"\"\n",
        "        animate function for FuncAnimation\n",
        "        \n",
        "        Input:\n",
        "            i : integer for indexing\n",
        "        \"\"\"\n",
        "        self.im.set_array(self.decision_boundaries[i])\n",
        "        \n",
        "    def get_animation(self):\n",
        "        \"\"\"\n",
        "        Return animation object which holds the animation\n",
        "        \"\"\"\n",
        "        return FuncAnimation(self.fig, self.animate, frames=len(self.decision_boundaries))\n",
        "\n",
        "def get_decision_boundary(data, classifiers, alphas):\n",
        "    \"\"\"\n",
        "    Create the decision boundary of a strong classifier. The function creates a 12 x 12 grid and a strong classifier. For every location in the grid the prediction is obtained and returned.\n",
        "    \n",
        "    INPUT:\n",
        "        data : 2D data points\n",
        "        classifiers : list of trained classifiers\n",
        "        alphas : ndarray with weights of the classifiers\n",
        "\n",
        "    OUTPUT:\n",
        "        decision_boundary : ndarray of shape (n_classifiers, 12**2) with prediction results for a (12 x 12) grid.\n",
        "    \"\"\"\n",
        "    \n",
        "    n_classifiers = len(classifiers)\n",
        "    \n",
        "    # create grid\n",
        "    nx = 12\n",
        "    ny = 12\n",
        "    x = np.linspace(-0.5, 0.5, nx)\n",
        "    y = np.linspace(-0.5, 0.5, ny)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "    xy = np.stack((xx.reshape(-1),yy.reshape(-1)), 1)\n",
        "    \n",
        "    # classify grid points\n",
        "    predictions = np.zeros((ny * nx, n_classifiers))\n",
        "    for i, classifier in enumerate(classifiers):\n",
        "        prediction = classifier.predict(xy)\n",
        "        predictions[:,i] = prediction\n",
        "    decision_boundary = ((alphas[None,:n_classifiers] * predictions).sum(axis=1) > 0).astype(int) * 2 - 1\n",
        "    decision_boundary = decision_boundary.reshape(ny, nx)\n",
        "    \n",
        "    return decision_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TnbgA8JSYm-",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Choose hyperparameters\n",
        "\"\"\"\n",
        "# TODO set number of samples (observations)\n",
        "n_samples = 2000\n",
        "\n",
        "# TODO set number of rounds\n",
        "n_classifiers = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gtd-LXpSYm_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Train, combine, and visualize multiple weak classifiers \n",
        "\"\"\"\n",
        "\n",
        "# TODO generate data\n",
        "data, targets = gen_data(n_samples)\n",
        "\n",
        "# TODO initialize sample weights\n",
        "weights = np.full(shape=(n_samples), fill_value=1/n_samples)\n",
        "\n",
        "# TODO initialize arrays or lists for storing the alphas, predictions, decision_boundaries, errors and classifiers\n",
        "alphas = np.zeros(n_classifiers)\n",
        "predictions = np.zeros((n_samples, n_classifiers))\n",
        "classifiers = []\n",
        "decision_boundaries = []\n",
        "errors = np.zeros((n_classifiers))\n",
        "\n",
        "for t in range(n_classifiers):\n",
        "    # TODO train weak classifier\n",
        "    classifier = Classifier()\n",
        "    \n",
        "    classifier = train_classifier(classifier, data, targets, weights)\n",
        "\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # TODO compute predictions of the weak classifier\n",
        "    predictions[:,t] = classifier.predict(data)\n",
        "    \n",
        "    # TODO calculate the weighted! training error of the weak classifier\n",
        "    error = 0\n",
        "    for i in range(n_samples):\n",
        "        if predictions[i,t] != targets[i]:\n",
        "            error += weights[i]\n",
        "\n",
        "    # TODO calculate alpha\n",
        "    alphas[t] = 0.5 * np.log((1-error)/error)\n",
        "\n",
        "    # TODO update weight distribution for samples (note: normalization)\n",
        "    for i in range(n_samples):\n",
        "        weights[i] = weights[i]*np.exp(-alphas[t]*targets[i]*predictions[i,t])\n",
        "\n",
        "    weights /= np.sum(weights)\n",
        "    \n",
        "    # TODO calculate the error of the final strong classifier (so far)  \n",
        "    tmpPredict = np.zeros(n_samples)\n",
        "\n",
        "    for T in range(t+1):\n",
        "        tmpPredict += alphas[T]*predictions[:,T]\n",
        "        \n",
        "    tmpPredict =  (tmpPredict > 0).astype(float)*2-1    \n",
        "    for i in range(n_samples):\n",
        "        if tmpPredict[i] != targets[i]:\n",
        "            errors[t] += 1\n",
        "    \n",
        "            \n",
        "    # TODO compute the decision boundary of the current strong classifier (so far) \n",
        "\n",
        "    decision_boundaries.append(get_decision_boundary(data, classifiers, alphas))\n",
        "    \n",
        "# TODO plot the error of the final strong classifiers for each round\n",
        "errors /= n_samples\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmH2tsbOSYnC"
      },
      "outputs": [],
      "source": [
        "animation = Animation(data, targets, decision_boundaries)\n",
        "HTML(animation.get_animation().to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj3U-VyVSYnF"
      },
      "source": [
        "## Exercise 3.2: Comprehension Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsOXSl-SYnF"
      },
      "source": [
        "Answer the following comprehension questions either with right or wrong and briefly explain your decision:\n",
        "\n",
        "1. The margin of a Support Vector Machine (SVM) is the distance of the nearest data point to the separating hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-O5JcUVSYnG"
      },
      "source": [
        "Your answer: True, the margin is the distance to the nearest point of both classes, since the hyperplane should seperate them in the best possible way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCVOjWlYSYnI"
      },
      "source": [
        "2. The support vectors are the data samples furthest away from the seperating hyperplane of a SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUE-OXYRSYnL"
      },
      "source": [
        "Your answer: Wrong, the support vectors are constructed by the nearst data samples to the seperating hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWeETqRrSYnL"
      },
      "source": [
        "3. All support vectors have the same distance to the seperating hyperplane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W113UHa0SYnM"
      },
      "source": [
        "Your answer: True, because the seperating hyperplane is in the middle of the support vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ3NGAF-SYnN"
      },
      "source": [
        "4. To maximize the margin in an SVM, you minimize the length of the weight vector (under the given constraints)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYKkIzySSYnN"
      },
      "source": [
        "Your answer: True, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkaA_NIBSYnO"
      },
      "source": [
        "5. Slack variables are introduced to allow for misclassification in a SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABo7hvkfSYnQ"
      },
      "source": [
        "Your answer: True, because this allows certain training points to be within the margin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyY2h5TuSYnR"
      },
      "source": [
        "6. The parameter C in the optimization of a SVM (c.f. equation 2.30) controls the tradeoff between maximization of the margin and minimizing the number of misclassified samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJth0QiVSYnR"
      },
      "source": [
        "Your answer: True, because the parameter is use for that case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsCf1-uESYnR"
      },
      "source": [
        "7. A small parameter C could lead to overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw1oksGWSYnR"
      },
      "source": [
        "Your answer: True, because it allows only a small amount of false classified data samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElUmfnXQSYnS"
      },
      "source": [
        "8. A SVM with the kernel $K \\left( \\vec{x}, \\vec{y} \\right) = \\langle \\vec{x}, \\vec{y}\\rangle^2$ can be used to separate the data given in task 1 (hint: see exercise 2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk_yJzp9SYnT"
      },
      "source": [
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WIIVxfySYnT"
      },
      "source": [
        "9. To seperate 5 different classes with a multi-class SVM, in total you need to train 5 SVM for one-versus-all and 9 SVM for one-versus-one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj6jAMz3SYnU"
      },
      "source": [
        "Your answer: False, a multi-class SVM needs 10 SVM for one-versus-one."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Kopie von exercise_4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
